{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Attempt to get TPU address from environment variable\n",
    "    tpu_address = os.environ['COLAB_TPU_ADDR']\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + tpu_address) # Fix typo: 'grcp' to 'grpc'\n",
    "    print(\"Running on TPU:\", tpu_address)\n",
    "except KeyError:\n",
    "    # If TPU address not found, use default strategy (CPU or GPU)\n",
    "    print(\"TPU not found, using default strategy\")\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Initialize without arguments for default behavior\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "# TPU Strategy 세팅\n",
    "strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 컴파일\n",
    "def create_model():\n",
    "  return tf.keras.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.Conv2D(256, 3, activation='relu', input_shape=(28,28,1)),\n",
    "          tf.keras.layers.Conv2D(256, 3, activation='relu'),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(256, activation='relu'),\n",
    "          tf.keras.layers.Dense(128, activation='relu'),\n",
    "          tf.keras.layers.Dense(10)\n",
    "      ]\n",
    "  )\n",
    "\n",
    "# with GPU\n",
    "# model = create_model()\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "with strategy.scope():\n",
    "  model = create_model()\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 정제\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "print('훈련용 리뷰 개수 :',len(train_data)) # 훈련용 리뷰 개수 출력\n",
    "print('테스트용 리뷰 개수 :',len(test_data)) # 테스트용 리뷰 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 데이터 및 결측치 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # 중복값 제거\n",
    "train_data = train_data.dropna(how = 'any')                   # null 제거\n",
    "print('훈련용 리뷰 개수 :',len(train_data))\n",
    "\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)  # 중복값 제거\n",
    "test_data = test_data.dropna(how = 'any')                     # null 제거\n",
    "print('테스트용 리뷰 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델 생성\n",
    "def convert_examples_to_features(sentences, labels, max_seq_len, tokenizer):\n",
    "  input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "\n",
    "  for example, label in tqdm(zip(sentences, labels), total=len(sentences)):\n",
    "    # input_id는 워드 임베딩을 위한 문장의 정수 인코딩\n",
    "    input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "\n",
    "    # attention_mask 설정 - 실제 단어 존재시 1, 패딩이면 0\n",
    "    padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "    attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "\n",
    "    # token_type_id는 문장을 구분하기 위한 인덱스\n",
    "    token_type_id = [0] * max_seq_len\n",
    "\n",
    "    assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "    assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "    assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "    data_labels.append(label)\n",
    "\n",
    "  # np.array로 변환\n",
    "  input_ids = np.array(input_ids, dtype=int)\n",
    "  attention_masks = np.array(attention_masks, dtype=int)\n",
    "  token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "  data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "  return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "max_seq_len = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "train_X, train_y = convert_examples_to_features(train_data['document'], train_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터\n",
    "max_seq_len = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "test_X, test_y = convert_examples_to_features(test_data['document'], test_data['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 첫 번째 샘플 출력\n",
    "input_id = train_X[0][0]\n",
    "attention_mask = train_X[1][0]\n",
    "token_type_id = train_X[2][0]\n",
    "label = train_y[0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :', tokenizer.decode(input_id))\n",
    "print('출력 샘플의 레이블 :',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT를 이용한 Many-to-one 모델\n",
    "class TFBertForSequenceClassification(tf.keras.Model):\n",
    "  def __init__(self, model_name):\n",
    "    super(TFBertForSequenceClassification, self).__init__()\n",
    "    self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "    self.classifier = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02), activation='sigmoid', name='classifier')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    input_ids, attention_mask, token_type_ids = inputs\n",
    "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    cls_token = outputs[1]\n",
    "    prediction = self.classifier(cls_token)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU Setting\n",
    "with strategy.scope():\n",
    "  model = TFBertForSequenceClassification('klue/bert-base')\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  loss = tf.keras.losses.BinaryCrossentropy()\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_X,train_y, epochs=2, batch_size=64, validation_split=0.2)  # 배치크기 64, validation 20%, 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋에 대한 loss, accuracy\n",
    "results=model.evaluate(test_X, test_y, batch_size=1024)\n",
    "print(\"test loss, test acc: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "# from keras.models import load_model  # This is not needed for saving weights\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save the entire model to a directory\n",
    "model.save('/content/drive/MyDrive/AIML/Codes/NLP/Modeling/KoBERT_Practice_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 예측해보기\n",
    "def sentiment_predict(new_sentence):\n",
    "  input_id = tokenizer.encode(new_sentence, max_length=max_seq_len, pad_to_max_length=True)\n",
    "  padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "  attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "  token_type_id = [0] * max_seq_len\n",
    "\n",
    "  input_ids = np.array([input_id])\n",
    "  attention_masks = np.array([attention_mask])\n",
    "  token_type_ids = np.array([token_type_id])\n",
    "\n",
    "  encoded_input = [input_ids, attention_masks, token_type_ids]\n",
    "  score = model.predict(encoded_input)[0]\n",
    "\n",
    "  # Extract the prediction score from the NumPy array (assuming it's the first element)\n",
    "  score = score[0]  # Access the first element to get the scalar value\n",
    "\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model test\n",
    "sentiment_predict(\"이 영화 존잼입니다 대박\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
