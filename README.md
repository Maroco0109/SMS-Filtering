# Spam SMS Filtering with Natural language model and Image processing

1. About Project

- NLPëª¨ë¸ë“¤ì„ í™œìš©í•˜ì—¬ ìŠ¤íŒ¸ ë©”ì„¸ì§€ë¥¼ í•„í„°ë§í•˜ëŠ” ëª¨ë¸ êµ¬í˜„
  - ìŠ¤íŒ¸ ë©”ì„¸ì§€ì— í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì„¤ì •í•˜ì—¬ KoBERT ëª¨ë¸ì„ í†µí•´ í•´ë‹¹ SMSê°€ ìŠ¤íŒ¸ ì¹´í…Œê³ ë¦¬ì— í¬í•¨ë˜ëŠ”ì§€ íŒŒì•…í•˜ì—¬ í•„í„°ë§
  - ì—¬ëŸ¬ í•œêµ­ì–´ ì²˜ë¦¬ pre-trained ëª¨ë¸ ì‚¬ìš© ë° ì„±ëŠ¥ ë¹„êµ ë¶„ì„(https://github.com/monologg)
  - KoBERT
  - KoELECTRA
  - KoRoberta
  - KoBigBird
- Sentiment Predict Example
  - ![Sentiment Predict Example](https://github.com/user-attachments/assets/094c3de1-eddc-4d16-b66e-29129824343b)

2. Project Setup

- Model Development Environmnet
  - Anaconda3 in Windows 11
    - GPU setup for deep learning(RTX 3060 12gb)
      - CUDA
      - CuDNN
  - Python
    - Pytorch
- Project architecture
  - ![SMS Filtering](https://github.com/user-attachments/assets/511bd687-edcf-4e68-bd1f-88dc86e59242)

## ğŸ“‚ Directory Structure

```bash
Spam SMS Filtering/
â”œâ”€â”€ data/ # ì›ë³¸ ë° ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥
â”‚ â”œâ”€â”€ raw/ # ì›ë³¸ ë°ì´í„°
â”‚ â””â”€â”€ preprocessed/ # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ preprocess/
â”‚ â”œâ”€â”€ build_dataset.py
â”‚ â”œâ”€â”€ preprocess.py
â”‚ â”œâ”€â”€ process_test.py
â”‚ â””â”€â”€ util.py
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ __init__.py
â”‚ â”œâ”€â”€ config.py # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ë§¤ê°œë³€ìˆ˜
â”‚ â”œâ”€â”€ data_util.py   # ë°ì´í„° í† í°í™”
â”‚ â”œâ”€â”€ logger.py   # Log íŒŒì¼ ìƒì„±
â”‚ â”œâ”€â”€ model_util.py  # í•œêµ­ì–´ LLM
â”‚ â”œâ”€â”€ data_preprocessing.py # ìŠ¤íŒ¸/í–„ ë°ì´í„° ì „ì²˜ë¦¬ ê´€ë ¨ ì½”ë“œ
â”‚ â””â”€â”€ predict_text.py   # ê°ì„± ë¶„ì„
â”œâ”€â”€ main.py # ë©”ì¸
â”œâ”€â”€ plm.py  # ëª¨ë¸ í˜¸ì¶œ ë° í›ˆë ¨
â”œâ”€â”€ dataloader.py # ë°ì´í„°ë¡œë”
â”œâ”€â”€ eval.py # ëª¨ë¸ í‰ê°€
â”œâ”€â”€ requirements.txt # í•„ìš”í•œ Python íŒ¨í‚¤ì§€ ë¦¬ìŠ¤íŠ¸
â”œâ”€â”€ main.py # ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md # í”„ë¡œì íŠ¸ ì„¤ëª…
```

## ğŸ“‹ Description

ì´ í”„ë¡œì íŠ¸ëŠ” **í•œêµ­ì–´ ì²˜ë¦¬ LLM**ì„ í™œìš©í•˜ì—¬ ìŠ¤íŒ¸ ë©”ì‹œì§€ì™€ ì •ìƒ ë©”ì‹œì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬, í•™ìŠµ, í‰ê°€ ê³¼ì •ì„ í†µí•´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ë©°, ê°ê°ì˜ ê³¼ì •ì€ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸš€ How to Run

1. **í™˜ê²½ ì„¤ì •(old)**:

```bash
pip install -r requirements.txt
```

2. ë°ì´í„° ì „ì²˜ë¦¬:

```bash
python utils/data_preprocessing.py
```

```bash
python build_dataset.py --split --data_dir ../data --save_dir ../result
```

```bash
python build_dataset.py --split --use_test --data_dir ../data --save_dir ../result
```

3. ëª¨ë¸ í•™ìŠµ:

### Electra

- Cross Entrophy

```bash
python main.py --train --data_dir result \
--model_type electra --model_name electra+revised --max_len 64 --gpuid 0
```

- Focal loss

```bash
python main.py --train --data_dir result \
--model_type electra --model_name electra+revised --max_len 64 --gpuid 0 --use_focal_loss
```

### Bert

- Cross Entrophy

```bash
python main.py --train --data_dir result \
--model_type bert --model_name bert+revised --max_len 64 --gpuid 0
```

- Focal loss

```bash
python main.py --train --data_dir result \
--model_type bert --model_name bert+revised --max_len 64 --gpuid 0 --use_focal_loss
```

### Roberta

- Cross Entrophy

```bash
python main.py --train --data_dir result \
--model_type roberta --model_name roberta+revised --max_len 64 --gpuid 0
```

- Focal loss

```bash
python main.py --train --data_dir result \
--model_type roberta --model_name roberta+revised --max_len 64 --gpuid 0 --use_focal_loss
```

### Bigbird

- Cross Entrophy

```bash
python main.py --train --data_dir result \
--model_type bigbird --model_name bigbird+revised --max_len 64 --gpuid 0
```

- Focal loss

```bash
python main.py --train --data_dir result \
--model_type bigbird --model_name bigbird+revised --max_len 64 --gpuid 0 --use_focal_loss
```

4. í…ŒìŠ¤íŠ¸:

### ë°ì´í„° ì¶œë ¥

```bash
python utils/test_case.py
```

### Bert

```bash
python main.py --pred --data_dir result \
--model_type bert --model_name bert+revised \
--model_pt model_ckpt/epoch=03-avg_val_acc=1.00.ckpt --max_len 64 --gpuid 0
```

```bash
python utils/predict_text.py --model_type bert --model_pt model_ckpt/epoch=03-avg_val_acc=1.00.ckpt --gpuid 0
```

### Electra

```bash
python main.py --pred \
--data_dir result \
--model_type electra \
--model_name electra+revised \
--model_pt model_ckpt/epoch=01-avg_val_acc=1.00.ckpt \
--max_len 64 --gpuid 0
```

ğŸ“¦ Requirements
ì´ í”„ë¡œì íŠ¸ì—ì„œ í•„ìš”í•œ Python íŒ¨í‚¤ì§€ëŠ” requirements.txtì— ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
pip install -r requirements.txt
```

Phase 1: Model Conversion and Optimization

1. Convert PyTorch Model to Mobile Format

   - Convert the KoBERT model to TorchScript format
   - Optimize the model size using quantization
   - Export the model in a format compatible with PyTorch Mobile

2. Prepare Model Assets
   - Export the tokenizer vocabulary and configurations
   - Create a lightweight version of the model suitable for mobile devices
   - Package model files and assets for Android integration

Phase 2: Android App Development

1. Project Setup

   - Create a new Android project in Android Studio
   - Set up the required dependencies:
     ```gradle
     dependencies {
         implementation 'org.pytorch:pytorch_android:1.13.0'
         implementation 'org.pytorch:pytorch_android_torchvision:1.13.0'
         // Add other necessary dependencies
     }
     ```

2. App Architecture(example)

   - by Kotlin or Dart

   ```
   app/
   â”œâ”€â”€ java/
   â”‚   â”œâ”€â”€ activities/
   â”‚   â”‚   â”œâ”€â”€ MainActivity.java
   â”‚   â”‚   â””â”€â”€ ResultActivity.java
   â”‚   â”œâ”€â”€ model/
   â”‚   â”‚   â”œâ”€â”€ SpamPredictor.java
   â”‚   â”‚   â””â”€â”€ KoBertTokenizer.java
   â”‚   â””â”€â”€ utils/
   â”‚       â””â”€â”€ TextPreprocessor.java
   â”œâ”€â”€ assets/
   â”‚   â”œâ”€â”€ spam_model.pt
   â”‚   â””â”€â”€ vocab.txt
   â””â”€â”€ res/
       â””â”€â”€ layout/
           â”œâ”€â”€ activity_main.xml
           â””â”€â”€ activity_result.xml
   ```

3. Core Features Implementation
   - Create text input interface
   - Implement real-time SMS monitoring (optional)
   - Build prediction result display
   - Add message history feature

Phase 3: Model Integration

1. Port the Prediction Logic

```java
public class SpamPredictor {
    private Module model;
    private KoBertTokenizer tokenizer;

    public SpamPredictor(Context context) {
        // Load the model
        model = Module.load(assetFilePath(context, "spam_model.pt"));
        tokenizer = new KoBertTokenizer(context);
    }

    public PredictionResult predict(String text) {
        // Tokenize and predict
        // Return spam probability and classification
    }
}
```

2. Implement Tokenizer

```java
public class KoBertTokenizer {
    // Port the Python tokenizer logic to Java
    // Implement necessary preprocessing
    public IValue tokenize(String text) {
        // Convert text to model input format
    }
}
```

Phase 4: User Interface Development

1. Main Screen

   - Text input field
   - "Check Message" button
   - History of recent checks
   - Settings button

2. Result Screen
   - Classification result (Spam/Ham)
   - Confidence score
   - Detailed analysis
   - Action buttons (Report false positive/negative)

Phase 5: Additional Features

1. Real-time SMS Monitoring

   - Background service for SMS monitoring
   - Notification system for spam detection
   - Auto-categorization of messages

2. Settings and Customization
   - Sensitivity adjustment
   - Notification preferences
   - Language settings
   - Theme options

Phase 6: Testing and Optimization

1. Performance Testing

   - Model inference speed
   - Memory usage
   - Battery consumption

2. Accuracy Testing
   - Test with various message types
   - Validate Korean language handling
   - Check edge cases

Phase 7: Deployment and Maintenance

1. Release Preparation

   - App optimization
   - Documentation
   - Privacy policy
   - Play Store listing preparation

2. Post-Release
   - Monitor performance
   - Gather user feedback
   - Plan updates and improvements

Timeline Estimation:

- Phase 1: 1-2 weeks
- Phase 2: 1 week
- Phase 3: 2 weeks
- Phase 4: 1 week
- Phase 5: 2 weeks
- Phase 6: 1 week
- Phase 7: 1 week

Total estimated time: 9-10 weeks