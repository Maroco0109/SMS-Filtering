import torch
from transformers import AutoTokenizer

# ëª¨ë¸ ê²½ë¡œ
model_paths = {
    "bert": "result/model/bert_bertrevised.pt",
    "electra": "result/model/electra_electrarevised.pt",
    "roberta": "result/model/roberta_robertarevised.pt"
}

# tokenizer ë¡œë“œ (ëª¨ë¸ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ, ìš°ì„  bert ê¸°ë°˜ tokenizer ì‚¬ìš©)
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)

# ëª¨ë¸ ë¡œë“œ
models = {}
for name, path in model_paths.items():
    models[name] = torch.jit.load(path, map_location='cpu')
    models[name].eval()

def predict(text, model, tokenizer):
    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
    input_ids = tokens['input_ids']
    attention_mask = tokens['attention_mask']

    with torch.no_grad():
        outputs = model(input_ids, attention_mask)

    probs = torch.softmax(outputs, dim=-1)
    spam_prob = probs[0][0].item() * 100  # 0ë²ˆ í´ë˜ìŠ¤: ìŠ¤íŒ¸
    ham_prob = probs[0][1].item() * 100   # 1ë²ˆ í´ë˜ìŠ¤: í–„

    return spam_prob, ham_prob

if __name__ == "__main__":
    print("âœï¸ í‚¤ë³´ë“œ ì…ë ¥ì„ í•´ì£¼ì„¸ìš” ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ)")

    while True:
        user_input = input("\n> ì…ë ¥ ë¬¸ì¥: ")

        if user_input.lower() == "exit":
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        for model_name, model in models.items():
            spam_prob, ham_prob = predict(user_input, model, tokenizer)
            print(f"\n[{model_name.upper()} ëª¨ë¸ ê²°ê³¼]")
            print(f"  ğŸ“Œ ìŠ¤íŒ¸ í™•ë¥ : {spam_prob:.2f}%")
            print(f"  ğŸ“Œ í–„ í™•ë¥ : {ham_prob:.2f}%")
