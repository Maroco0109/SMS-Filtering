import sys
import os
import argparse

# ê²½ë¡œ ì„¤ì •: í”„ë¡œì íŠ¸ ìƒìœ„ í´ë”ì— plm.py, utils/ ë””ë ‰í† ë¦¬ê°€ ìžˆë‹¤ê³  ê°€ì •
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
from utils.model_util import load_model
from utils.data_util import encode

def predict_input_pt(text, model, tokenizer, device, max_len=64):
    """
    PyTorch ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ë¡œ ì˜ˆì¸¡
    """
    model.eval()
    model.to(device)

    with torch.no_grad():
        # Hugging Face ìŠ¤íƒ€ì¼ ì¸ì½”ë”©
        input_ids, attention_mask = encode("[CLS] " + text + " [SEP]", tokenizer, max_len=max_len)
        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)
        attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)

        # forward
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs
        pred = torch.argmax(logits, dim=-1).item()
        return pred

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_type', type=str, default='bert',
                        help='bert / electra / roberta ë“± ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--model_pt',   type=str,
                        help='PyTorch ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (.ckpt)')
    parser.add_argument('--onnx_model', type=str,
                        help='ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.onnx)')
    parser.add_argument('--max_len',    type=int, default=64,
                        help='í† í° ìµœëŒ€ ê¸¸ì´')
    parser.add_argument('--gpuid',      type=int, default=0,
                        help='GPU ID (CUDA ì‚¬ìš© ì‹œ)')
    args = parser.parse_args()

    # ONNX ëª¨ë“œ
    if args.onnx_model:
        import numpy as np
        import onnxruntime as ort

        # ONNX InferenceSession ìƒì„±
        session = ort.InferenceSession(args.onnx_model)
        input_names  = [inp.name for inp in session.get_inputs()]
        output_name  = session.get_outputs()[0].name

        # í† í¬ë‚˜ì´ì €ë§Œ ë¡œë“œ
        _, tokenizer = load_model(args.model_type, num_labels=2)

        print("ðŸ“¢ ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ë¬¸ìž¥ì„ ìž…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ìž…ë ¥")
        while True:
            text = input("ìž…ë ¥ > ").strip()
            if text.lower() == 'exit':
                break

            # encode â†’ NumPy 2D ë°°ì—´(int64)
            input_ids, attention_mask = encode("[CLS] " + text + " [SEP]", tokenizer, max_len=args.max_len)
            feed = {
                input_names[0]: np.array([input_ids], dtype=np.int64),
                input_names[1]: np.array([attention_mask], dtype=np.int64),
            }

            # ONNX ì¶”ë¡ 
            logits = session.run([output_name], feed)[0]  # shape: (1, 2)
            pred   = int(np.argmax(logits, axis=-1)[0])

            label_map = {0: "Spam ðŸ“›", 1: "Ham âœ…"}
            print(f"ðŸ¤– ì˜ˆì¸¡ ê²°ê³¼: {label_map[pred]}")

    # PyTorch ì²´í¬í¬ì¸íŠ¸ ëª¨ë“œ
    else:
        from plm import LightningPLM

        # device ì„¤ì •
        device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() else 'cpu')

        # HuggingFace ëª¨ë¸ + í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer = load_model(args.model_type, num_labels=2)
        args.num_labels = 2

        # LightningPLMì—ì„œ í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        plm_model = LightningPLM.load_from_checkpoint(checkpoint_path=args.model_pt, **vars(args))
        model = plm_model.model.to(device)

        print("ðŸ“¢ PyTorch ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ! ë¬¸ìž¥ì„ ìž…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ìž…ë ¥")
        while True:
            text = input("ìž…ë ¥ > ").strip()
            if text.lower() == 'exit':
                break

            pred = predict_input_pt(text, model, tokenizer, device, max_len=args.max_len)
            label_map = {0: "Spam ðŸ“›", 1: "Ham âœ…"}
            print(f"ðŸ¤– ì˜ˆì¸¡ ê²°ê³¼: {label_map[pred]}")

if __name__ == '__main__':
    main()
